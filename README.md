# HPC-Twitter-GeoProcessing

1. The task of the projec:
  In this project, the task is to implement a parallelized application to search tweets that are located around Melbourne in a large Twitter dataset and indentify tweet hotspots based on the searching results. Python is used to achieve the goals.

2. Python application to achieve the twitter processing:
  The dataset has a size of roughly 10GB so it is too large to read in a single time. In this section, a Python script is implemented to address this issue and do the searching.
  In the task, two files are provided. They include the Twitter file and Grid file which contains information about the latitudes and longitudes of grid boxes of interest.To handle the Twitter file, what the Python script does is to read the file line by line. Once a line is read, the script identifies it and stores the result locally. After that, the script will move on to the next line and repeat the process till reaching the end of the Twitter file. To read the grid file, it is much simpler due to its small size. Instead of reading this file line by line, the script reads the entire file at once and parses coordinate information about the grid boxes.
  One feature, that in the file, every line is a tweet, except the first one and the last one, makes it possible to parse the information of coordinates directly without parsing the whole JSON record of a tweet. Regular expression (as shown below) is used to locate and retrieve coordinates with the help of the re library in Python. Once a line is read from the Twitter file, the script retrieves the coordinate information and then it identifies its location.
  Identifying the location of a tweet is straightforward. The script compares the latitude and longitude with the maximum and minimum latitudes and longitudes of every grid box. If the tweet is not in grid boxes of interest, it will be ignored. To record the amount of tweets in grid boxes, a list is utilized as the data structure to store the identification results.
  In order to analysis the performances of different running configurations, the time library is imported to provide a method to obtain the beginning time and finishing time of the whole searching. And the execution time is the difference between these points in time.

3. Method to parallelize the job:
  In the requirements, the Python script will be run on different resources, involving 1 node with 1 core, 1 node with 8 cores, and 2 nodes with 8 cores. Thurs, it needs to parallelize the job to make full use of resources. In case of 1 node and 1 core, the entire procedure is described in Section 2, but the implementations in the cases of 1 node with 8 cores, and 2 nodes are different mainly in the process of each line in the Twitter file. When more than 1 core is assigned, one core is acting as the Master who is responsible to read the Twitter file line by line. Once it reads a line from the Twitter file, it sends that line to one other core and moves on to the next line of the file. The remaining cores act as Workers and their job is to parse coordinate information from the lines they receive from the Master core, identify the location of each particular tweet and store the identification results locally. To gather the process results, the Master core will send a “None” message to all other cores when it reaches the end of the Twitter file. When Worker cores finish the process of all lines they get and receive the “None” message, they send their local process results back to the Master cores. The Master core sums up all the results to get the final result of the searching job.

4. SLURM scripts for running jobs on Spartan:
  In order to request different number of nodes and cores to run the Python, the parameters: ‘nodes’, ‘ntasks’ and ‘ntasks-per-node’, are needed to be setup in the different SLURM scripts. To run the Python script parallel, Python model is loaded and ‘srun’ command is used, instead of ‘mpirun’.
